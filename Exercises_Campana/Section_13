1.3 IO Latencies


Exercise 1.3.1: Latencies for remote IO

time and strace the “curl” command to retrieve the CERN main page home.web.cern.ch. Use
the strace option to display the real time as the first column (-ttt) and restrict the syscall output to
network related calls (-e network).

$ curl home.web.cern.ch

Identify the network latency and explain the execution time of the command.

Hint “time strace -ttt -e network curl home.web.cern.ch”

SOLUTION

Simply enter

> time strace -ttt -e network curl home.web.cern.ch

From the `man strace' page, some description of the used options.

* -ttt: Prefix each line of the trace with the time of day. 
	If given thrice, the time printed will include the
        microseconds and the leading portion will be printed as the
        number of seconds since the epoch.

* -e: 	   A qualifying expression  which  modifies  which  events  to
           trace  or  how to trace them.  The format of the expression
           is:

                   [qualifier=][!][?]value1[,[?]value2]...

           where qualifier is one of trace, abbrev, verbose, raw, sig‐
           nal,  read,  write,  fault, or inject and value is a quali‐
           fier-dependent symbol or number.  
		   
	   In particular:
	   -e trace=network (deprecated)
           Trace all the network related system calls.

So we are tracing network related system calls, prefixing every line (which means every network related system call) with the time in microseconds.
Then, we take the time of the whole strace command. We get the following result at the end:

real	0m1,168s
user	0m0,047s
sys	0m0,046s

If we take the real time (time elapsed from pressing the `enter' key to the completion of the command) and subtract the times used in the processing
modes (user and sys times) we get the time spent not computing (which I interpret as the time spent waiting for the connection).
We get 1,075 seconds.

######################################################################################################

Exercise 1.3.2: Latencies for Remote IO

You have straced an analysis application using files on your local hard disk. You see that it
issues synchronous read calls with the syscall statistics output as shown:

% time 	seconds	 usecs/call	 calls	 errors	 syscall	
19.30 	0.039031 1		 20000 		 read
14.61 	0.000143 143 		 1 	 1	 connect

The realtime of the application running with local data is 10 seconds and it is CPU bound.
Assuming that the application uses the same IO model for remote file IO, which runtime do you
expect for the application to read a file from a remote location with a network RTT of 20ms.

SOLUTION

If the network RTT (round trip time, the length of time it takes for a signal to be sent plus the length of time it takes for an acknowledgement of that signal to be received) is equal to 20 ms; if the same IO model is used for both local and remote, then, to each of the 20000 read calls must be
added a 20 ms time. This ends up in 20000*20 ms = 400 s for the read operations. So we expect ~410 s as the time needed for the remote operation.

######################################################################################################

Exercise 1.3.3: Latencies for Remote IO
The ROOT application uses an optimization called TTreeCache to improve remote data access.
Taking into account exercise 1.3.2. which kind of optimization could that be? What do you need
to do to reduce latency effects?

SOLUTION

A possible solution is offered by asynchronous pre fetching. We could get an idea of the files number/file size in order to issue multiple read calls
at once. This way, the `overhead' of 20 ms would only be experienced once. This way, CPUs would not experience waiting times after finishing
their computations.




FROM ROOT DESCRIPTION ABOUT TTREECACHE
(Motivation: why having a cache is needed?
When writing a TTree, the branch buffers are kept in memory. A typical branch buffersize (before compression) is typically 32 KBytes. After compression, the zipped buffer may be just a few Kbytes. The branch buffers cannot be much larger in case of TTrees with several hundred or thousand branches.

When writing, this does not generate a performance problem because branch buffers are always written sequentially and, thanks to OS optimisations, content is flushed to the output file when a few MBytes of data are available. On the other hand, when reading, one may hit performance problems because of latencies e.g imposed by network. For example in a WAN with 10ms latency, reading 1000 buffers of 10 KBytes each with no cache will imply 10s penalty where a local read of the 10 MBytes would take about 1 second.

The TreeCache tries to prefetch all the buffers for the selected branches in order to transfer a few multi-Megabytes large buffers instead of many multi-kilobytes small buffers. In addition, TTreeCache can sort the blocks to be read in increasing order such that the file is read sequentially.

Systems like xrootd, dCache or httpd take advantage of the TTreeCache in reading ahead as much data as they can and return to the application the maximum data specified in the cache and have the next chunk of data ready when the next request come)









