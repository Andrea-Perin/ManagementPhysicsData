\documentclass[]{suhbook}
\usepackage[backend=biber,style=alphabetic,refsection=chapter]{biblatex}
%% biber processes the reference db
%% citation keys and entries will be alphabetic
%% A reference section will be added to each chapter

\title{Management of physics datasets}
\author{...}
\date{...}
\begin{document}

\setChapterVersionDate{}
\setChapterAuthor{}
\setcounter{chapter}{1}% To jump to a chapter number, N-1 here
\chapter{Campana's part}
\subsection{Introduction}
When dealing with physics experiments the likes of those performed at CERN, the events we are looking for have ridiculously low probabilities. The production of just $\approx$ 10 Higgs required $10^11$ billion collisions. Such a huge number of events obviously makes their management hard.
The detector sees about 50 simultaneous collisions happening at a rate of 30 MHz. This is a huge data volume for the ATLAS detector to read out. Taking also into account that each collision produces data of about 1.5MB in size, this would correspond to 45 TBytes of data produced per second. This is a huge number of events to store, but also to process. Furthermore, most of these events are not really interesting for analysis. So we need a way to select which of all these 30MHz of events to keep. 
This is done through a sort of filtering process, that progressively reduces the amount of data, going from the detector to the publication.
An event's lifetime goes as follows:
\begin{enumerate}
    \item Detector;
    \item Trigger;
    \item Data preparation, reconstruction and calibration;
    \item Data analysis;
    \item Publication.
\end{enumerate}
An additional, somewhat parallel step, is the one related to simulations. 
Triggering is the first processing step of the event that is created at the center of the detector after the collision has occurred. If the trigger decides for the event to be accepted for further processing, the event moves to disk. The data get reconstructed and calibrated such that we can trust what we are measuring. On parallel, we generate simulated data that describes what the standard model signals should look like on the detector, or how new physics signals should look like. We feed our detector data and our simulated data to analyses that we develop, often using sophisticated analysis techniques. The results of these analyses is what makes our physics publications. 
Triggering is essential in reducing the magnitude of the data stream coming from the detector. LHC collisions clock in at $\approx$40 MHz, and the triggering procedure allows us to get a more manageable $\approx$1 kHz. This triggering system is actually composed of two separate stages: a first layer, with  selection logic encoded on fast electronics, provides a less than 0.5\% acceptance. The data is then passed on to a large CPU farm, the High Level Trigger, which is running fast algorithms to take the final decision. In the end about 1 in 30’000 events are stored for further processing.
The trigger helps the selection of interesting events, but also applies upper limits in the number of events we can select at each of its steps. 
After the detector+trigger system has completed its job, RAW data is produced. These will later be made available to analysis after adequate preparation.
An interesting aspect of this schema is the time every step takes. Collisions occur every 25 ns. The hardware trigger takes 2.5 μs to process the event and decide whether to keep it or not. The HLT takes about 300 ms. This constitutes what we call the “online” processing, and it effectively happens in “real time”. Anything beyond that processing step is happening “offline”. In general, later steps take much more time than the ones at the beginning.
This is where high efficiency computing (HEP) steps in. The main purpose of computing facilities is to make RAW data into usable data, by means of reconstruction, calibration and Monte Carlo simulations. This whole offline part converges to the final step of actual data analysis.
\subsubsection{Reconstruction}
The event reconstruction turns RAW data into Analysis Object Data (AOD). (something about event reconstruction).
The actual reconstruction involves tracks and clusters:
\begin{itemize}
    \item tracks: the measurement of particles' trajectories inside a detector are subject to a degree of uncertainty. The smaller the errors, the more constrained the possible trajectories are. More points and smaller errors reduce the possible trajectories. In order to find them, least squares procedures are employed, so that uncertainties about the trajectories themselves can be retrieved, too;
    \item clustering: by clustering, we mean the procedure of reconstructing the amount of energy that gets deposited in the calorimeter by the particles that traverse it. The meaningful quantities are: the energy, the position of the deposit, and the direction of the incident particles. Typically, the shower produced by one or more particles extends over contiguous cells (the basic units of which a calorimeter is composed). There are various clustering algorithms that can group various cells based on the expected behavior of a shower pattern.
\end{itemize}
Combining these yields a so called object (which one could also call a particle): trajectories and energy measurement, grouped together.
\subsubsection{Monte Carlo simulations}
At this point, the data is ready for the actual analysis. However, at this level, Monte Carlo simulations come into play. This is because we only have one detector: how do we know that the effects it measures are due to its specifics and construction? Maybe, a different detector would have produced different results. And what about the detector's interactions with radiation? 
Moreover, when we measure something through a detector, the measures we get are voltages, currents, times. Linking them to a specific particle set, however, is an interpretation. Simulating the detector itself can aid in correcting inefficiencies, inaccuracies and so on. 
Monte Carlo simulations are used as a way to get the expected values from theory, which can then be compared to the experimental values. They are also fundamental in order to prove that the detector's working is known to the experimentalists.
The chain of Monte Carlo simulations works as follows:
\begin{itemize}
    \item Event generation (from <1s to a few hours per event): simulating the actual physical process through which the particles are produced;
    \item Detector simulation (from 1 to 10 minutes per event): simulating the interactions of the particle with the materials that compose the detector;
    \item Digitization (from 5 to 60 s per event): translating the interactions into realistic signals;
    \item Reconstruction: analogously to what happens for real data, the events must be reconstructed.
\end{itemize}
This simple overview is enough to realize how computationally demanding the whole procedure is. There is 1 PB/s of data generated by the detectors, up to 60 PB/year of stored data. Some large experiments have managed data sets of more than 200 PB. This equates to one million cores working 24/7. This amount of processing and storage, if centralized, would place CERN amongst the top supercomputers in the world. However, the system is actually distributed.
\subsubsection{The GRID model}
The GRID model is a computing model that involves a distributed architecture of a large number of (loosely) coupled computation and storage centers, which are connected in order to solve a complex problem. GRID is organized in multiple tiers, levels with different sizes and different tasks.
\begin{itemize}
    \item Tier-0: Located at CERN. Involves data recording, reconstruction and distribution;
    \item Tier-1: permanent storage, re-processing, analysis. Tier-1 sites are located in multiple research facilities around the world ($10^1$ as order of magnitude);
    \item Tier-2: these smaller sites (around 160 of them) are focused on simulations and end-user analysis.
    \item Tier-3: smaller institutes;
    \item Tier-4: single workstations.
\end{itemize}
In total, there are ~170 sites in 42 countries, ~750k CPU cores, ~1 EB of storage ($10^1^8$ byte), >2 million jobs per day and 10-100 Gb links. An international collaboration for the distribution of LHC data exists in the Worldwide LHC Computing Grid (WLCG), which integrates different computing centers around the world under a single infrastructure accessible to all involved physicists.
The fact that the whole system is distributed makes so that there is a continuous data transfer at rates of 35GB/s across the WLCG.
The HL-LHC is expected to produce ~1 EB of data by 2026. How can such a massive amount of data be managed? There is a need for a computing model: GRID itself is a computing model, a distributed one at it.
\subsection{Data management components}
Scientific computing is based on a distributed architecture. This type of architecture is based on three main pillars:
\begin{itemize}
    \item Computing power;
    \item Storage;
    \item Links (network).
\end{itemize}
These three are deeply interconnected, and are limited by budget. ANy approach that aims at optimising the performance/cost of a model must optimize with all three aspects in mind. Having low CPU usage may be good, nut not if it requires a lot of memory/network costs.
In general, the main problems which data management faces are the following:
\begin{itemize}
    \item Data reliability: the data must be self consistent over time and protected from spoil;
    \item Access control: it is important to decide who can actually access the stored data. There is the need for an authentication protocol;
    \item Data distribution: there must be a way for data to be transferred to whoever may require them;
    \item Data archives, history, long term preservation: simply put, the data must not be modified as time passes. The way to achieve this is to resort to tapes, which make data reachable and readable;
    \item In general, a data management protocol must allow the establishment of a workflow for data processing, covering all steps that are needed from data collection to end analysis.
\end{itemize}
\subsubsection{Storage models}
The simplest version of a storage model is storing all data together in one place. Its advantages are uniformity (everything is done in the same way), simplicity, ease of maintenance and no need to move data. This means that performances and reliability can be quite good. However, this is far too simplistic for a real life situation.
Something similar happens on the commercial side of things. Google Drive, Dropbox etc. are simple cloud-based systems which procide the user with two main services:  get and put (and also some additional ones, like sharing and so on). In case of need, additional services may also be purchased. Such a model works because it receives a lot of funding, since the companies are private: this is a commercial model. Each Petabyte on Dropbox costs 10 times its "physical amount": the additional price comes from all the services built around the storage itself. Needless to say, physics needs something with a much better storage/cost ratio. Moreover, the requirements for the storage largely depend on what physics we are dealing with.

For a statistical mechanics application, for instance: the starting point is a bunch of configuration files, each of small size (approximately 1MB), which are then fed to some computationally intesive procedure (typically, simulations). This is a CPU bound operation. When it comes to storage, we do not need something too massive, as the files are not straining for the storage. However, we must be sure that the configuration files are preserved. To do this, we may employ a large numebr of disks. We need to achieve
\begin{itemize}
    \item redundancy;
    \item reliability.
\end{itemize}
For a high energy physics application, instead, the most demanding part is the storage one. We deal with large volumes of data, over which we need to perform a small amount of relatively fast operations: the operation is IO bound (input/output bound: a large fraction pof time is spent on accessing data). In this case, what we need is actually a few copies with high performance. Ina sense, we are asking the dual version of the storage needed before.
This is where multiple data pools come into play. We need a system that is suitable for both requirements; to achieve such flexibility, the preferred way is to employ a number of specialised structures (pools). Indeed, the two building blocks to empower data processing are:
\begin{itemize}
    \item Data pools with different quality of services (defined on the basis of the particular need of the process we are considering);
    \item Tools for data transfer between pools.
\end{itemize}
Data pools themselves have three main parameters, which can be conveniently placed as the vertices of a triangle. These are performance, reliability and cost. Only two of these can be achieved at a time. As examples, flashdrives, solid state disks (SSDs) and mirrored disk are fast and reliable, but expensive. On the contrary, tapes are extremely cheap and reliable, but lack in speed (due in part to their sequential nature). Disks, instead, are cheap and fast, but unreliable. As a consequence, depending on the service one needs, an appropriate mix of these must be employed. Actually, the triangular model is an approximation, as more categories can be evaluated as a metric for comparing different hardware choices. For instance: where is scalability?
\subsection{Data management components}
\subsubsection{Name servers and databases}
The name server is the database which contains the catalogue of all data. In a sense, it acts like the index of the greater dataset. It is a simple lookup based, single key database application that can be implemented in various ways. However, it is the most complex and error prone component, while also being the most important. Losing one disk of the database is not tragic: data can be retrieved. However, losing the name server means losing all the information. As a consequence, reliability is critical, and the name server must be accessible at all times. Furthermore, because of its role, the name server is potentially a bottleneck in the overall performance. When a server is not able to provide all requested information, it enters a denial of service (DoS) state. A good name server is one which, when the maximum number of requests is reached, does not immediately enter DoS, but instead manages incoming requests and leaves them pending until they can be processed. On the other hand, for a bad name server, reaching the maximum means shutting down the whole system.
An improvement of performance can be attained by employing an indexed lookup table and returning the requested data in multipl batches (paging in shards). This is because, for requests that involve a large fraction of all entries in the database, a complete fulfillment of the request may result in having the server locked in that particular high weight task. 
An example of a well designed name server system are the uniform resource identifiers (URI): for example, the address

\verabtim{http://csc.cern.ch/data/2012/School/page.htm}

can be decomposed in successive parts, following a sort of hierarchical reasoning. At first, we define the protocol ('https'), then the host/domain ('csn.cern.ch'), then a particular volume ('data') and so on. This is an example of a federated namespace, which offers almost infinite horizontal scalability and high efficiency, but is weak to DNS failure: if the DNS fails, no information can be retrieved at all.
A similar problem arises in storage systems:

\verbatim{storage://cern.ch/data/2012/School/page.htm}

where the database lookup is located at the file level, which is the last one (in this case, 'page.htm'). This allows a greater flexibility, but it impacts the performance. In general, the two extremes are:
\begin{itemize}
    \item file systems (i.e., no database)
    \item storage systems with lookup tables at the file level
\end{itemize}
and many possible compromises exist between the two.
\subsubsection{Data access protocols}
The starting point in data access protocols is POSIX (Portable Operating System Interface for Unix): this is a standard which (among other things) defines file-level access: opening, reading, writing, deleting and so on. In posix, metadata is limited: there are users, permits but not much else. This, together with the fact that some operations (like ls)are not scalable, make posix a sub optimal choice for distributed data. However, there are some commands which perform as if they were posix but are suitable for distributed data.
The concept of distributed data is also linked to that of efficiency. IMagine we had to perform the sum of two number on disk. We would need to perform the following operations:
\begin{itemize}
    \item open the file in the disk;
    \item allocate memory in the RAM in order to store the two numbers;
    \item Move them in the CPU register and sum the two number s there;
    \item transfer the result back in the memory, and from there to the disk if necessary.
\end{itemize}
This simple example highlights an important concept, that is data locality. When data and computing power (i.e., CPUs) are co-located, the efficiency can be high. Whenever a site has idle CPUs (i.e., not enough data), idle or saturated networks, or an excess of data (which implies that there are not enough CPUs for analysis), then efficiency drops. Indeed,
$$
efficiency = \frac{\text{time during which CPUs work}}{\text{total run time of the program}}
$$
Then, limiting the idle time of CPUs improves performance. This is why analysis made with high efficiency requires the data to be pre-located to where the CPUs are available, or to allow peer-to-peer data transfers, so that sites with excess CPUs can process excess data. Both these approaches are used in HEP.
\subsubsection{I/O performance}
When dealing with distributed data, the measurement and general assessment of IO performance is fundamental. There are 4 main observables that summarize the IO performance: 
\begin{itemize}
    \item Bandwidth (IO volume/time);
    \item IOPS (IO operations/time);
    \item latency (the signal delay, time/IO operation);
    \item blocksize (payload/operation).
\end{itemize}
The last item is linked to the way a typical OS works: files are organized in blocks of a certain size, and each block is transferred in a single operation.
IO can be synchronous or asynchronous. In synchronous IO, each process (or thread), after entering the IO operation state, enters a wait state until completion of the operation. In asynchronous, the process does not wait: instead, it sends the kernel an IO request. Once the kernel completes the IO operation, the process is notified.
Another possible distinction is between local and remote IO, and sequential vs. random. Sequential means that the reading occurs on contiguous memory registers, while random implies no particular order. Generally speaking, random access is slower than sequential access. Even when drawing randomly, it is usually better to do so in order.
As examples:
\begin{itemize}
    \item end user analysis: employs local and synchronous IO;
    \item video streaming, bulk data analysis: sequential and synchronous IO;
    \item big data analysis: remote, asynchronous IO;
    \item selective data analysis: random access, asynchronous IO.
\end{itemize}
Different types of memory have different performances on the 4 metrics introduced before. In general, performance wise, 
$$
\text{Tape<Disk<SSD<RAM memory}
$$
However, the downside of RAM lays not only in its cost but also on the fact that it is volatile: it only exists as long as the process (in general, the OS) is active.
Some useful Linux commands for checking IO performance are:
\begin{itemize}
    \item `time'
    \item `dd': copy/block IO tool;
    \item `vmstat -n 1'
    \item `iostat -x 1'
    \item `dstat'
    \item `top', `htop', `iftop', `iotop': task overview, a sort of task manager
    \item `strace <program> [program args]': trace system commands of a program. If `-c' is added, then it counts the system calls.
    \item `sync': flush dirty pages as root or user;
    \item `echo 3 > /proc/sys/vm/drop\_caches': clean the cache
\end{itemize}
Some commands:
\begin{itemize}
    \item baseline performance: $$ \text{dd if=/dev/zero of=/dev/null bs=1M count=16000}$$
    \item writing to a hard disk: $$ \text{dd if=/dev/zero of=/data01/16G bs=1M count=16000}$$
    \item reading from a hard disk: $$ \text{dd if=/data01/16G of=/dev/null bs=1M count=16000}$$
    This command results in really low times. This is due to the fact that the data has been saved in the cache, which makes accessing it much quicker. If we want to know the actual speed, we first need to clear the cache, and then perform the reading again:
    $$ \text{echo 3 > /proc/sys/vm/drop_caches} $$
    $$ \text{dd if=/data01/16G of=/dev/null bs=1M count=16000} $$
    There are two main caching strategies: write-through and write-back. In write through, the cache is used only after the changes have been made permanent (acknowledged). In write-back, instead, the changes are stored in the cache, and are acknowledged from time to time. Of the two methods, write-back is faster, as data does not have to be saved on disk each time it is modified; however, it is also a riskier approach, as any failure (e.g., a simple power outage) may erase any change that occurred after the last commit to the disk. Linux based systems use the write-back paradigm: as a consequence, if we want to make sure that a change has been saved in the disk, it is not sufficient to re-open the file after the modification, but we must look at it from the OS.
    \item taking a look at how the blocksize impacts actual performance: by trying
    $$ \text{dd if=/dev/zero of=/dev/null bs=N count=1000}$$
    with N in [1,10,100,1000,10000,1000000] we can take a look at how the bandwidth (MB/s) varies. For both bandwidth and IOPS, there is a bottleneck somewhere in the set of N.\\
    For increasing blocksizes, the bandwidth increases accordingly, while the IOPS decreases.
\end{itemize}
It is also important to take into account how latency impacts the analysis of data. Take ROOT as an example. Here, the software issues thousands of small read requests to iterate over data structures inside ROOT format files. What if such an application had to retrieve that data from a remote location? Then latency may become highly impactful, and reduce the IO efficiency. ROOT itself employs various techniques to reduce the impact of such latency.
Let us use the process of ordering a beer as an analogy:
\begin{itemize}
    \item uncompensated: this is the easiest way to act, since no particular countermeasure against high latency is taken. This is the best way to act when a fast disk is available. This is the method used by Python notebooks, and this is why notebooks are not used in a HEP context.
    \item pre-fetching: carrying one beer to a table may take a long time. It is possible to cut on the overall waiting time by simply carrying around more than one at a time. This way, the waiting is divided among multiple orders. If the waiter knew how many beers each customer wanted, he could bring more than one at a time and then improve the efficiency. The same is done by ROOT, which can `learn' each user's preferences over time, and optimize itself accordingly. In a sense, pre-fetching requires some previous knowledge.
    \item asynchronous pre-fetching: as soon as a beer (the data on a disk) arrives, a customer may ask for another one. Then, the time spent on drinking (processing the data) would partially overlap with the time required for the second beer to arrive. This way, efficiency is improved without requiring previous knowledge.
\end{itemize}
\subsection{Reliability}
Reliability is related to the probability of losing data. A possible definition is "the probability that a storage device will perform an arbitrarily large number of IO operations without data loss during a specified period of time". Reliability can be referred to both hardware and services, but the fundamental one is hardware related. For example, on a disk server with simple disks, reliability of the service is just the reliability if those disks. Data management involves finding ways to improve reliability, even if this means an increasing cost and/or additional hardware (e.g., disk mirroring and RAID).
Hardware reliability is based on tapes. These have a bad reputation in some use cases, as they are slow in random access mode (they are intrinsically sequential) and can have high latency when mounting processes and seeking data (rewinding and fast-forwarding are rather slow), are inefficient for small files and are comparable to disks when looking at their cost. They however have also many advantages, like a fast sequential access mode (2x the sequential speed of disks), a much higher reliability (several orders of magnitude higher than disks), no need of energy for storing the data (unlike disks, which must be powered in order to store anything), a better physical volume per petabyte ratio, and are less prone to accidental erasing of large amounts of data in a small time. This means that, when not used in random access mode, tapes serve a precise role in the architecture.
When discussing reliability and disks, the redundant array of inexpensive disks (RAID) system plays a fundamental role. There are many types of RAIDs:
\begin{itemize}
    \item RAID0: disk striping. Data is distributed between two or more disks, but with no  redundancy. This increases the performance, but decreases reliability (so that it is not a proper RAID, as there is no redundancy).
    \item RAID1: disk mirroring. This is the opposite of RAID1, as the same data is simply copied on two disks, so that they are mirrored. If one particular piece of data is lost, then its copy on the other disk will prevent any actual loss.
    \item RAID5: parity information, distributed across all disks. 
    \item RAID6: Reed-Solomon error correction (allowing up to 2 disks lost without actual loss of data)
\end{itemize}
Errors can be detected if there is a checksum: data should be consistent with the respective checksum (so that errors may be detected independently). By using redundancy, then, errors can be corrected. 
Reed-Solomon error correction is based on oversampling a polynomial constructed from the data by a margin of n points. Since any set of m distinct points can univocally determine an m-1 degree polynomial, and since that there are m+n points, then we can afford to lose up to n points without actual loss, as the original polynomial can still be retrieved from the remaining points. For different values of n, we have different implementations, as n=0 equals no redundancy, n=1 is the RAID5 architecture and n=2 is the Reed-Solomon one. 
With n corrupted values, the Reed-Solomon procedure can only detect an error, while for lower values of n it can also be corrected. However, if a checksum/hash is added on each point, then we can correct also n errors.
With RAID, reliability depends on various parameters, namely the hardware reliability, the type of RAID and the number of disks in the set. 
\subsubsection{RAID5 reliability}
In RAID5, disks are regrouped in sets of the same size, with each disk having capacity $c$. If there are $n$ of such disks, then the RAID5 capacity will be $c(n-1)$, so that the system is immune to the loss of one disk, but not of two. 
Some calculations: disks mean time between failures (MTBF) is between $3\times10^5$ and $1.2\times 10^6$ hours, replacement time of a failed disk is less than 4 hours. Then, the probability of a disk to fail within the next 4 hours (meaning that there are two failed disks at the same time, and so total loss of data) is
$$
P_f = \frac{Hours}{MTBF}=\frac{4}{3 \times 10^5} = 1.3 \times 10^{-5}
$$
Then, the probability of at least one disk failing in the next 4 hours inside a 15PB computer centre (that is, a centre with 15000 disks) is
$$
P_{f_{15000}}=1-(1-P_f)^{15000}=0.18
$$
In a 10-disks RAID5 system, the probability of one of the remaining disks failing within 4 hours is
$$
P_{f_{9}}=1-(1-P_f)^{9}=1.2\times 10^{-4}
$$
which is obviously smaller. However, we can not assume that the second failure is independent of the first one: after all, the two disks are in the same computer centre. We can then try and arbitrarily increase this number by two orders of magnitude just to take dependencies into account, so that we get
$$
P_{f_{9corr}}=1-(1-P_f)^{900}=0.0119
$$
Then, the probability of losing the data stored in the computer centre over the next 4 hours is
$$
P_{loss}=P_{f_{9corr}} \times P_{f_{15000}}=6.16\times 10^{-4}
$$
which may seem small. However, over a 10 year period, this becomes
$$
P_{loss10y}=1-(1-P_{loss})^{10\times 365 \times 6}\approx 1
$$
\subsubsection{RAID6 reliability}
What about RAID6? In this architecture, disks are regrouped in sets of arbitrary size. If each disk has capacity $c$ and there are $n$ disks per set, then the total capacity will be $c(n-2)$, meaning that the loss of up to 2 disks is allowed.
As before, the probability of a disk to fail within the next 4 hours is 
$$
P_f = \frac{Hours}{MTBF}=\frac{4}{3 \times 10^5} = 1.3 \times 10^{-5}
$$
In a set of 10 disks, the probability of one of the remaining 9 disks failing is
$$
P_{f9}= 1-(1-P_f)^{900}=1.19\times 10^{-3}
$$
and the probability of one of the remaining 8 failing is
$$
P_{f8}= 1-(1-P_f)^{800}=1.06\times 10^{-3}
$$
Then, the probability of losing data in the next 4 hours is 
$$
P_{loss}=P_{f9}\times P_{f8} \times _{f_{15000}} = 2.29 \times 10^{-5}
$$
and over 10 years, this becomes
$$
P_{loss10y}=1-(1-P_{loss})^{10\times 365\times 6}=0.394
$$
which is better than its RAID5 counterpart.
\subsubsection{Arbitrary reliability}
Since RAID systems are disk based, it means that the smallest unit is just a disk. This is a alck of granularity: it would be a more flexible solution to have smaller sizes. This can be achieved by using file chunks (or blocks): by splitting files in chunks of fixed size $s$, and grouping them in sets of $m$ chunks, generating $n$ additional chunks in a Reed-Solomon like fashion and scattering them across independent storages will allow us to retrieve all the information in the $m$ chunks, provided that up to $n$ have been lost or corrupted (in a sense, chunks are treated as disks).
Acting this way, we can achieve arbitrary reliability, as the system is immune to the loss of $n$ simultaneous failures from pools of $m+n$ storage chunks. This, however, results in a raw storage space loss of $\frac{n}{m+n}$. A higher $m$ results in a smaller space lost, but at the cost of a higher data loss in the case of failure.
This is analogous to gambling: a big $m$ may seem a good choice, until a failure actually happens, and a large portion of data gets lost. On the other hand, arbitrarily high reliability can be achieved by setting a small $m$, but at the cost of a lot of space lost.
The values of $m$ and $n$ can be tuned in the following way:
\begin{itemize}
    \item starting from the MTBF provided by the hardware builder, decide the (somewhat arbitrary) replacement time of a failed component;
    \item choose $m$ and $n$ as to expect the wanted reliability;
    \item while running the system, constantly monitor and record the hardware failures and the replacement times, as to constantly tweak and adjust both MTBF and the average replacement time;
    \item recalculate $m$ and $n$ using these constantly adjusted values.
\end{itemize}
\subsubsection{Chunks transfer}
If we want to use chunks, it is necessary to define how these chunks are transferred. There are many protocols, among which the most popular is BitTorrent. For each chunk, an SHA1 hash (a 160 bit digest, i.e. the output of a cryptographic hash function, which maps any input to a fixed-size binary string in such a way that obtaining the input from the output is extremely hard) is created, and all digests are assembled in a single torrent file, which contains all relevant metadata information. Torrents are published and registered, with a tracker that lists all clients currently sharing the torrent's chunks. In particular, they contain an announce section, which specifies the URL of the tracker, and an info section, containing names of the files, their length and the list of SHA1 digests. It is then up to the client to retrieve and reassemble the original file, while also checking its integrity.
In order to do so, a hash is associated to each chunk; this guarantees that errors can be corrected also in the case of a loss of $n$ chunks (analogously to what happens when applying checksums in Reed-Solomon). The overhead of the checksum is negligible when compared to the actual chunk size, since a checksum is around a few hundred bytes and a chunk is various megabytes. Still, if we want high efficiency, the physical block size of the storage must fit both the chunk and its checksum. Having chunks the exact size of the physical block is an error, as it leaves no space for the checksum in the same block, and so the chunk will actually require 2 physical blocks instead of one.
\subsubsection{A small recap of arbitrary reliability}
\begin{itemize}
    \item Plain: reliability of the service is just reliability of the hardware.
    \item Replication: reliable and with maximum performance; however, requires a lot of storage overhead, as data is doubled. For example, with 3 copies, there would be a 200\% overhead.
    \item Reed-Solomon, triple parity: maximum reliability, minimum storage overhead. For instance, a 4+2 system can afford a loss of 2 (and so has a storage overhead of 50\%); a 10+3 can lose 3 (and so has a storage overhead of 30\%)
    \item Low density parity check (LDPC): this approach uses a bipartite graph where the original data is one layer, and the other one is composed of the additional copies. <some other stuff about it maybe later>
    \item File checksums and block level checksums
\end{itemize}
\subsubsection{Availability vs.  reliability}
Reliability is linked to the probability of losing data. However, data can be not available even without it being actually lost: for example, the service may be down and the data temporarily unavailable. This is to say that high reliability does not imply high availability and viceversa. How can we guarantee both reliability and availability? 
Take the following example: suppose $T$ independent storage sets, and have the storage being configured to replicate files $R$ times, with $R<T$. For example, we may write each file a total of $R=3$ times across a system of 6 storage units. Any of those three may be used by a client when reading, and the load can be spread across the whole system, so that a higher throughput may be attained. Now, suppose that a failure in one of the storage units is detected. The unit is automatically disabled. Read requests can still continue, as long as there is at least one copy of the requested file in the system. Also, file creation can still take place, as newly written data will be placed in one of the remaining 5 units. Delete requests will still work, and updates can continue, just by modifying the remaining copies or creating an additional copy on the fly on another unit. Thus, even in case of hardware failure, service can still continue. The broken part can then be substituted by a brand new component, on which data can be copied starting from the replicas that were not damaged.
Some calculations: since there is redundancy, there must be multiple failures in order for data to be lost. 
$$
Prob(\text{Service fail})=Prob(\text{failure of 1 and failure of 2})
$$
If the two events were independent, then we would be able to factorize the joint probability and calculations would be easy. However, that is not always the case: we then need to reduce the dependencies as much as possible. A distributed/dispersed storage on different servers (or even better, on different data centers) can make different units independent. This greatly improves reliability, and allows the aforementioned factorization:
$$
Prob(\text{Service fail})=Prob(\text{failure of 1})\times Prob(\text{failure of 2})
$$
In the case of $R$ replicas, then, the calculation is simply
$$
Prob(\text{Service fail})=Prob(\text{Single failure})^R
$$
So, knowing the probability of a single failure, we can calculate the probability of data being unavailable by raising it to the $R$-th power.
When actually dealing with managing such a storage system, it is important to have enough spare parts to cope with both planned replacements and unexpected hardware failures. Furthermore, the system should not be dependent on timely repair, so that only asynchronous interventions are required (otherwise, there would be the need of having a constant presence of on-site technicians to face failures). In general, the responses that can be automated should be automated, and manual operations should be kept at a minimum. Also, reliability, availability and performance should be tuned as to meet the service agreements.
\subsection{Access control and security}
Data management is deeply linked to cryptography, as it enables control over data access.
\subsubsection{Cryptography: symmetric and asymmetric encryptions}
Cryptography solves the following needs:
\begin{itemize}
    \item confidentiality: ensure that nobody can access the contents of the transferred information, even if the whole message was to be listened to;
    \item integrity: ensure that the message has not been modified during transmission;
    \item authenticity: we can verify that the entity we think we are talking to is actually who we think it is;
    \item identity: we can verify who the specific individual behind the entity is;
    \item non-repudiation: the individual can not deny his association with the entity he's behind.
\end{itemize}
There are two main types of encryption: symmetric and asymmetric.
\begin{itemize}
    \item symmetric encryption: an input is encrypted by using a certain key, thus producing an encrypted version. If the recipient of the message knows the key, then he can decrypt the encoded message and get the information. This means that in symmetric encryption schemes, both the sender and the receiver (and no other individual!) must know the shared key. The key becomes a common secret between the two.
    \item asymmetric encryption: in this paradigm, the encryption and the decryption use different keys. There is no need for the sender and the receiver to share a common, secret key. Instead, each of them possess two keys, one of which is public (i.e., everyone knows it) while the other is private (only the owner knows it). It is important to stress that, in asymmetric encryption, the relation between the two keys (private and public) is unknown, and no knowledge of one key can be retrieved from the other, even if both the clear-text input and the encrypted text are available at the same time. Also, the two keys must be interchangeable: when a key pair (i.e., a public-private pair) is generated, any of the two can be used as public or private (at least, in theory).
\end{itemize}
It is important to stress that the two approaches differ also in what cracking them means. For an asymmetric encryption scheme, cracking means solving a well defined associated mathematical problem, which is usually something like `find $x$ and $y$ such that $x\times y=N$, with an extremely large $N$'. 
In symmetric encryption, instead, cracking is performed by trying different decryptions and comparing the results with some supposed decryption. Since the message is not known, there is actually no `ground truth', but we will only consider those decryptions which produce a meaningful result. If a decryption attempt of the code `fdT57Kb.Ph' yields `njcdipolz2', then we will be inclined to discard such attempt. In a way, there is the need of a sort of dictionary. In general, a guess of the original message will rely on supposed redundancies, pieces of message that, repeating themselves, give rise to possible weaknesses. From an encryption perspective, then, compression is important as it removes these redundancies, and leaves brute force attacks as the only possible way to crack the code.
Asymmetric encryption can provide a way to ensure authenticity and confidentiality. For authenticity, it is sufficient for the sender to encrypt some message with his private key. Then, the only valid decryption will be the one provided by the sender's public key. Then, authenticity is attained. Confidentiality, instead, is obtained by encrypting the message using the recipient's public key. This encrypted message is now decryptable only by the recipient himself, as he is the only one who owns the respective private key. 
\subsubsection{Cryptographic hash functions and digital signatures}
An additional tool in cryptography are cryptographic hash functions (CHF). These are a family of functions which, when fed some input string, transform it into a fixed-size output string (called `digest'), which can be thought of as a short representation of the original message. They are made in such a way that any small modification in the original input results in massively different digests. Among their requirements, hash function must be efficiently computable, and at the same time it should be impossible to find a (previously unknown) message that matches a given digest, and there should be no collisions (i.e., two different input strings producing the same digest).
In a sense, cryptographic hash functions are built in order to be easy to compute but extremely hard to invert, meaning that retrieving the input that produced a given digest should be virtually impossible. The `no collisions' requirement is actually not possible to satisfy. We are asking the function to be injective, but if every message (of arbitrary length) is transformed into a fixed-size digest, it can not possibly have this property (if the digest were 10 bits long, every possible 11 bit input should be mapped to the $2^{10}$ possible digests, which clearly invalidates any injectivity claim).
CHFs can be used in order to create a digital signature, and therefore allow integrity. Take for example a given message produced by the user A. By using a CHF, A can generate a digest of the message, then encrypt it using his/her private key. The now encrypted digest will serve as a digital signature. When the message is sent by A, the receiver can generate a digest for the message.Then, by using A's public key, the receiver can also decrypt the signature itself, thus recovering the original digest that A produced at the time of sending. Comparing the two (the digest produced by hashing the message and the one retrieved from decrypting the signature) allow the receiver to check the integrity of the message. If any modification was to be made on the message halfway through its transmission, then the two would almost certainly be different, while a perfect accordance would (almost certainly) imply that the message was not altered. Practically speaking, since collisions are extremely rare, digital signatures can be considered a proof of integrity.
An example of actual protocol is SSL (here presented in a simplified way). It ensures confidentiality, and, if digitally signed, it also ensures integrity. Also, depending on how the public keys are exchanged, it can ensure authenticity, identity and non-repudiation. Suppose A is the sender and B is the receiver, and both have their own private and public key. A initially encrypts the message using his private key, $Priv_A$, and then encrypts once again using B's public key, $Pub_B$. The doubly-encrypted message is transmitted over the public network, and B receives it. Now, since B is the only one who can `remove' $Pub_B$, as he is the only owner of $Priv_B$, he can turn back the message to its $Priv_A$ encryption. By using $Pub_A$, then, he can retrieve the original message.
In real world applications, however, SSL uses a hybrid encryption scheme, i.e., a mix of symmetric and asymmetric encryption. At first, a symmetric key is randomly generated, and it will be used strictly for the current session. This key will be used to encrypt the clear text message. Then, the symmetric key itself is encrypted, using the public keys of the recipients of the message. The symmetric-encrypted message will then be sent together with a set of `digital envelopes', each corresponding to a particular recipient. Each recipient will then be able to take `his' digital envelope and decrypt it using his/her private key, thus gaining access to the session key and decrypting the message.
Some final words about cryptography:
\begin{itemize}
    \item Kerchoff's principle: the security of the encryption scheme must depend only on the secrecy of the key, and not on the secrecy of the algorithm;
    \item on the same note, the algorithms should actually be published, and proved to be hack-resistant for quite some time;
    \item even the most advanced designs have some degree of weakness, and attacks can be successful: this must be taken into consideration, as there is no perfectly safe option.
\end{itemize}
\subsubsection{AAA: authentication, authorization, accounting}
Cryptography solves the aforementioned problems. However, how can we safely share secret keys (for symmetric encryption) and public keys on the internet? Take, for instance, the following situation: B creates a public/private pair of keys, and tells everybody that the public key he generated belongs to A. People will then send confidential information to A, encrypting with B's public key. A will not be able to decrypt these information, as he does not have the required private key. B, on the other hand, will be perfectly able to decrpyt the messages and access the confidential data. This is not due to a problem in asymmetric encryption itself, but rather a problem in the steps before encryption: nobody can guarantee that the keys B produced are actually his. 
There is the need of a trusted third party that can guarantee each other's identities. Some options are PKI (for asymmetric encryption) and Kerberos (for symmetric encryption).
\subsubsection{PKI and certificates}
If we wanted to exchange public keys with A, we would have one of two options: before using the key, meeting/calling A and check that the keys are right (for example, one could send a mail or make a telephone call, provided that he trusts these services), or we could also use a trusted third party and having it check if the keys are correct (here, certificates play a big role).
PKI (which stands for public key infrastructure) provides the technologies to enable practical distribution of public keys by using certificates. PKI is a group of solutions for key generation, key distribution, certificate generation, key revocation/validation and managing trust.
A certificate is a file which, in the simplest case, contains just a public key and information about the entity that is being certified to own that public key. Everything is then digitally signed by someone trusted (a friend or a certificate authority, or CA) and for which we already have a public key. The idea is that, when verifying a certificate, we take the contents and generate the hash, we decrypt the signature using the CA public key and compare the two (analogously to what was described above). If the two correspond, then the certificate is verified.
A common type of certificate is the X.509 certificate. It contains the following information:
\begin{itemize}
    \item X.500 subject: who is the owner;
    \item public key (or info about it);
    \item X.500 issuer (the entity which has signed);
    \item expiration date;
    \item additional arbitrary information;
    \item the CA digital signature (of the issuer of the certificate).
\end{itemize}
The idea is that, when verifying a certificate, we may end up `walking the path' of certifications, as an authority may be subordinate to another one issuing the certificate. This continues until we reach the root or an explicitly trusted subordinate CA. The point id that trust `is originated' at some point in the chain, and certificates will, sooner or later, reference that trusted entity.
Certificates allow authentication, but not in an automatic way. Owning a certificate of individual A does not mean we are individual A - we are not necessarily authenticated. For authentication to take place, some sort of challenge must be issued. In particular, if we wanted to verify that a person pretending to be A is actually A, we need to ask that person something that only the true A would know: that is, A's private key.
Person B gets person A's certificate, and then verifies the digital signature (meaning that he can now trust that the public key really belongs to person A). B then generates some random phrase, and challenges A to encrypt it. If A is really A, then he owns the appropriate private key that matches the public one stored in the certificate. B can then decrypt the message with that public key: if the decrypted phrase matches the staring one, then A is authenticated. 
NB: each time a private key is used to encrypt anything, it is indirectly exposed. It is a good idea to limit this exposure: to do so, key hierarchies and proxy certificates are used.
A proxy consists of a new certificate and a private key. The key pair that is used for the proxy, i.e. the public key embedded in the certificate and the private key, may either be regenerated for each proxy or obtained by other means. The new certificate contains the owner's identity, modified slightly to indicate that it is a proxy. The new certificate is signed by the owner, rather than a CA. 
The less a key is exposed, the longer it can stay active. Thus, the key from a `central' authority (for instance, CERN Root CA) should be kept as protected as possible, and `peripheric' keys should instead take the exposure. That is, the key of a single person should not be held in the same regard as the one from a central authority (i.e., there are hierarchical differences).
Certificates only contain public information and they are digitally signed, so there is no need of particular security: they hold no sensitive information and are protected from tampering by the digital signature anyways. The same is not true for private keys, however: they are confidential and must be stored in a particularly safe place (files protected by passwords, OS protected storage, smartcards). 
Smartcards can be ranked on the basis of their security: a bad smartcard is not smart at all, only containing the certificate and the private key, with no security mechanism to protect the private key. A better smartcard would make the private key not readable, only allowing the encryption of random strings using the private key (as a way of challenging
the knowledge of the private key itself). An even better smartcard would require the user to enter a PIN code before allowing any operation involving the private key, as to minimize unnecessary exposure of the private key.
The fact is that private key do indeed become compromised with the passing of time. As a consequence, certificates can be revoked. This can be done by requiring the CA to sign a certificate revocation certificate; still, the owner of the certificate must let the world know that the revocation actually comes from him: a difficult task, for which certificate revocation lists (CRLs) are used. This is a non scalable process: the CRL must be kept up to date, and mst be checked by the certificate validation process. An alternative is using expiration dates.
When the expiration date is reached, the CA issues a new certificate, choosing one of two approaches:
\begin{itemize}
    \item creating a new certificate and keeping the same public key, so that the user can keep on using his old private key;
    \item forcing the certificate to have a different public key, and thus forcing the user to change his private key too.
\end{itemize}
The choice depends on the intended purpose of the certificate (which is written in the certificate).
The underlying theme in all of these discussions is trust. In the real world, trust is easily rankable: we trust a passport way more than a random membership card. When it comes to digital certificates, however, this is not as easy. For instance, there are many classes of certificates with different purposes: a class 2 certificate is designed for people publishing software as individuals, while a class 3 certificate is designed for companies which publish software (and a class 3 certificate offers greater assurance, but also requires a minimum financial stability level).
\subsubsection{Kerberos: an alternative to PKI}
Kerberos has the same goals of PKI, but achieves them in a different way, which has both advantages and disadvantages. 
Pros:
\begin{itemize}
    \item Simpler to manage, keys are automatically managed, easier for users to understand;
    \item forwardable authentication is easier to implement.
\end{itemize}
Cons:
\begin{itemize}
    \item Cross domain authentication and domain trust are more difficult to implement;
    \item offline authentication is difficult to implement.
\end{itemize}
Kerberos is based on symmetric encryption (and so is completely different from PKI already).
It is based on the existence of a central authority, known as the Key Distribution Center (KDC). Each user shares a secret key with the KDC, through which secure communication with the KDC is possible: two different users have no way of verifying each other's identities. The KDC is trusted by everyone: its job is to distribute a unique session key to a pair of users that want to establish a secure channel. This way, the two users can use a symmetric encryption paradigm.
An example of a Kerberos session could be the following: person A wants to communicate with person (or service) B. A can communicate safely with the KDC by using his master key, let us  call it $M_A$, and can issue a communication request to B. The KDC then generates a unique, random key for A and B, let us call it $K_{AB}$. Two copies of $K_{AB}$ are sent to A: one is encrypted using A's master key, $M_A$, while the other is sent with A's name encrypted with B's master key; this is known as the `Kerberos ticket'. 
This ticket is actually a message to B: it notifies B that A wants to communicate. Since the value of $K_{AB}$ is only known to A, B and the KDC, if the entity that wants to communicate with B proves knowledge of this key, then B can safely assume it is actually A.
Now, authentication could be done in the following way: A sends the ticket to B, who decrypts it and gets $K_{AB}$. Now, B can generate some random phrase and send it back to A. A will then encrypt the phrase using $K_{AB}$ and send the encrypted phrase back to B, who can now verify if the encryption was done using the right $K_{AB}$; if that is the case, A is authenticated. The same process could then be repeated from A's perspective in order to authenticate B. However, Kerberos does not work in this way: the challenge is not simply a random phrase, but it is instead based on current time.
This is because of the possibility of a reflection attack: 
\begin{enumerate}
    \item a client (the victim) initiates a connection to the server (the attacker);
    \item the attacker initiates a new connection to the victim;
    \item now, the victim issues a challenge for the inbound (arriving) connection from the attacker;
    \item the attacker can `recycle' this same challenge back to the victim: this is because another connection is already open, and the attacker has still to issue a challenge
    \item the victim computes the correct response to the challenge posed by the attacker. In a sense, the victim responded to its own challenge;
    \item since the attacker now knows the correct answer to the challenge, he can use it to respond to the original challenge posed by the victim. The attacker has now access to the victim.
\end{enumerate}
In order to prevent this, Kerberos acts in the following way:
\begin{itemize}
    \item A sends the ticket to B;
    \item A has to prove that he knows $K_{AB}$. A's name and the current time are also sent, encrypted with $K_{AB}$ (this is called `authenticator');
    \item B takes the ticket, decrypts it and retrieves $K_{AB}$. Using it, it decrypts the  authenticator and compares the name in the authenticator to the name in the ticket.
    \item if the time is correct, this provides a proof that the authenticator was indeed encrypted with $K_{AB}$;
    \item B can also avoid attackers by rejecting authenticators with time already used or with the wrong time altogether.
\end{itemize}
The reason behind the encryption of time is that it serves as a way to prove the knowledge of the common secrete, that is, the common key $K_{AB}$. This implies that the time synchronization is fundamental for Kerberos to work properly. If B wants to avoid replaying an earlier attempt, he needs to keep track of all previous times. However, this approach does not scale. The alternative is to use a time window: B only needs to remember the times in the past (for instance) 5 minutes. Then, comparing the encrypted time with the recorded times of the past 5 minutes, B can be sure that the time has not been used before: the request can be accepted. However, if the time falls out of this window, B can not be sure that the time he is receiving has not already been used. As a consequence, the request is not granted. However, this provides a hint of what B's time is.
Now that A has proved his identity to B, A wants B to prove his own identity. To do so, A can send this via a flag in his request. After B has authenticated A, he takes the timestamp sent by A, encrypts it with $K_{AB}$, and sends it back to A. Now, A can decrypt this and verify that it is the same timestamp A originally sent. By doing so, A has authenticated B because only B could have decrypted the authenticator (as only B has the required master key, $M_B$. Recall that B receives both a ticket, encrypted with $M_B$, and an authenticator, which is encrypted with $K_{AB}$. Only B can actually retrieve $K_{AB}$ by decrypting the ticket!). By sending the timestamp back, encrypted with $K_{AB}$, B has proved he can access the information inside the authenticator. The time is the only piece of information that is unique in A's message.
After all these steps, A and B share a secure key $K_{AB}$ which they can use to communicate.
Actually, though, Kerberos is more complex, including an additional extra step. When A first logs in,  he actually asks the KDC for a ticket granting ticket, or TGT. Stored inside the TGT is the session key, $K_{AK}$, to be used for communications between A and the KDC throughout the day. So, when A requests a ticket for B, she actually sends the KDC her TGT plus an authenticator. The KDC then sends back the session key $K_{AB}$, encrypted using $K_{AK}$ (so the master key is not actually used).
This introduces a sort of hierarchical system of keys:
\begin{itemize}
    \item short-term key: the session key, a key shared among two entities for authentication purposes. Generated by the KDC, it is always sent on a secure channel, as the security of the whole Kerberos system depends on it: it is encrypted using the Ticket Granting Services (TGS) key.
    \item medium-term key: the TGS key, a secret key shared between entities and the KDC in order to obtain session keys. It is encrypted using the master key.
    \item long-term key: the master key, a secret key shared between each entity and the KDC. It must be known to both the entity and the KDC before the Kerberos protocol can take place. It is generated at the moment of the domain enrollment process and it is derived from the creator's password. It is sent through the secure channel.
    \item the secure channel: provided by the master key shared between the workstation and the KDC (in which case it is derived from the workstation's machine account password).
\end{itemize}
As alreaddy mentioned in the case of PKI keys, the keys with the longest lifetime are the ones which are less exposed. This makes the secure channel the most long term and the session key the shortest.
<slide 133...>
\subsubsection{Authorization}
Authorization implements the access control, that is, the inforamtion describing what the end-user can actually do on computing resources. It is the association of a right, a subject and a resource:
\begin{itemize}
    \item right: use, read, modify, delete, open, execute...
    \item subject: person, account, computer, printer, room...
    \item resource: file, computer, printer, room, information system...
\end{itemize}
Of course, an authorization can be time dependent; for example, it could be restricted to a window of some hours.
The authorization process is the verification that the user has all the required permissions to access a given resource.
In practice, each resource has a linked list called Access Control List (ACL). This is part of the metadata regarding that resource, and it is composed of Access Control Entries (ACEs). Every ACE contains the subject (person, account, computer, group...) and the right (use, read, and so on) that the subject has on the resource. The time of the right may also be present.
There are two main ways of using ACLs:
\begin{itemize}
    \item only `allow' permissions are possible;
    \item both `allow' and `deny' are possible.
\end{itemize}
In the second case, additional rules must be defined in order to avoid contradictions when dealing with groups of users (for instance, such rules may be that denies come first and ACEs are processed in the order they are defined).
Authorization must also take into account the identification of users. There are two main ways to do so:
\begin{itemize}
    \item identifying users by `login names' or by the `subject' found in the certificate;
    \item identifying by a unique GUID or Virtual ID, such that `login name' and `subject' only become two attributes of the account.
\end{itemize}
The latter is to prefer, as it has much better performance and is more flexible. However,since GUID/Virtual ID are instance-specific, moving the data requires modifications to the information.
There is a fundamental problem about authorization: where should the ACL be stored? On a database or within the resource itself? 
And how should ACL be verified? It can be delegated to the OS: the ACL are set on the file system and the process accessing the resource impersonates the credential of the user. It can also be managed by the application itself: the permissions of the owner of the request must be verified by the Data Management software on every request.
Furthermore, ACL must be supported on every node of the file system: every folder! Then, inheritance must be handled properly via a set of flags. Also, calculating the resultant permissions in real time when a particular file is accessed is indeed possible, but requires low level code optimization. Otherwise, we must resort to compiling the resultant permissions (file permissions + inheritance permissions) with the file, which becomes inefficient when changing permissions.
In a sense, calculating permissions in real time allows for faster writing, while the other approach behaves better in reading. In real life, a mix is implemented.
But that's not all: what about groups of users, and roles inside a group? ACLs can be granted to aggregate groups of users. But then again, when should the group resolution be done? Either at runtime, when the resource is accessed (at the cost of additional database lookups: bad performance!), or at authentication time: that is, when the user logs in, an authentication token (which contains the login name and all the groups the user belongs to)is produced. The ACL is the compared to all the groups that particular user belongs to, and authorizations are computed accordingly. This is obviously better, but, in the case pf a change in the groups memberships, it requires to log back in (that is, a re-authentication process) in order to be effective.
And what is the scope of the group? Is it something local to the storage element? Local to the site? Or global? 
Another important concept in authorization is granularity: how far does the protection of ACLs go? Is it restricted to pool, directories, files, parts of files? The best way to act is to defining the security model in a proper and consistent way:
\begin{itemize}
    \item No access by default, to be overridden by `allow permission' ACLs;
    \item World access (everywhere) by default, overridden by `deny permission' ACLs;
    \item an arbitrary default access that can be overridden by either one of the `allow' and `deny' ACLs (this one requires some more attention, as it must be able to handle permission conflicts in the case of inheritance and groups).
\end{itemize}
\subsubsection{Accounting: distributing responsibilities}
By accounting, we mean a list of actions (and related metadata, like who, when, what, where) that enable traceability of all subsequent changes and transactions rollback (i.e., enabling Ctrl+Z).
There may be multiple possible levels of accounting: simple logging (i.e., saving the history), logging and journal of all transactions (allowing to both know the history and perform rollbacks if necessary).
A good accounting strategy can be a valid alternative to a strict authorization scheme: users get more power, but also receive a share of responsibilities.
\subsection{Scalability}
\subsubsection{Cloud storage}
By cloud storage, we mean storage (generally hosted by third parties on the internet) that offers a model of virtual pools. This makes cloud storage highly scalable and flexible; also, it makes paying `a la carte' possible (only the needed/used services are actually paid for, everything is highly personalized, so to speak).
Cloud often employs simple interfaces to access storage, and does not employ the posix interface: this makes it possible to deploy scalable infrastructures (recall that posix, while otherwise good, does not allow scalability).
Cloud storage can indeed be simpler than traditional storage: there is a single pool where all data go, so that the system is fast, reliable and outsourced. There is a unique quality of service, and it is economically interesting for small/medium data sizes.
However, it can become simplistic: a single pool with a unique quality of service is not suited to the requirements of scientific computations, as it can easily become expensive and/or inefficient.
Cloud storage employs Distributed Hash Tables (DHTs) in order to empower scalability. The storage (that is composed of multiple clusters/data centers) is federated under a single name space.
Some keywords when discussing DHTs are hash tables (and distributed hash tables), hash algorithms and collisions. 
\begin{itemize}
    \item Hash table: a hash table is a data structure that uses a hash function to map keys (e.g., a person's name) to their respective values (e.g., the telephone number). A hash table implements an associative memory. A hash function is used to transform a key into the index of an array element where the respective value is stored. In a hash table, the cost for a lookup is independent of the table's dimension: this is a case of perfect scalability. Also insertions and deletions of key-value pairs can be performed at a constant cost, independent of the number of elements. 
    \item Hash algorithms and collisions: the hash table allows the retrieval, insertion and deletion of elements in a size-independent fashion. This is because the maximum number of entries in a hash table is determined by the chosen hash function. For example, using CRC16 as a hash function allows the creation of hash tables with $2^{16}$ (65536) entries, while SHA1 allows tables with $2^{160}$ (>$10^{48}$) entries. Whatever the size may be, hash functions are subject to collisions: a collision is the event of two (or more) distinct inputs (keys) being associated to a single output (value). Collisions must be handled appropriately. An important quantity when dealing with hash functions and hash tables is the Load factor (also known as fill factor), which is defined as the ratio between the number of stored entries and the size of the table's array. As the load factor increases, so do the probability of collisions and their cost. However, keeping the load factor too low results in wasted memory, as large portions of the hash table are left unused. The two extremes for the load factor (0 in the case of wasted memory, 1 in the case of too many collisions) are to be avoided. A way to do so is resizing the hash table: this requires a full or incremental rehash of all keys (for instance, rehashing may happen when the load factor goes over some pre defined threshold, say, 0.75. In this case, the table could for instance get doubled in size, and all hashes recalculated so that the entries spread over the entirety of the new hash table). Rehashing breaks scalability. A workaround is using a hash function that preserve the key hashes after the table gets resized. This is called consistent hashing. This becomes essential when dealing with distributed hash tables, since the load may be distributed among various servers, and adding/removing servers would otherwise require a complete rehashing of all stored data.
    \item Distributed hash tables: in order to hash among different servers, a keyspace partitioning scheme is used; it allows hash values to be distributed among the various servers. An overlay network is also established between all nodes in the network of servers, so that any one of them can route a certain data request (e.g., reading or writing some particular entry) to the server containing the right key. Once again, it must be stressed that the only viable option is consistent hashing, which allows servers to be added or removed and only affecting adjacent nodes. When implementing a distributed hash table, reliability and fault tolerance can be obtained by simply replicating values (or adding error correction information)on n adjacent nodes. In the case of widely spread nodes, the reliability can be calculated (as distances between data centers allow us to make the assumption of independence of failures). All of these features make distributed hash tables highly scalable, as data reading and writing are independent of the size of the hash table itself.
    \item Keyspace partitioning: two ways of performing partitioning are hash partitioning and range partitioning. Range requires a table through which objects are mapped to their storage instances, making adding instances easy (an example may be partitioning based on the date of the entry, so that temporally contiguous entries are stored in the same location). Hash partitioning requires all data to be moved when adding a new instance, unless consistent hashing is used (another possible workaround is starting with a large number of storage instances that are deployed on a few servers. When data grows, the self-contained instances can be moved to another server).
    The partitioning itself (i.e., associating the right entry to the right server) can be done in the client (if the client knows which server owns a particular key), in a middle-layer server (the so called proxy assisted partitioning: an intermediate database knows or calculates which server owns a key) or in the server instances (in the case that only the server knows or can calculate who owns a key; in this case, the clients send their queries to a random node, and the overlay network then manages the request and forwards it to the right server).
\end{itemize}
In practice, DHT implements storage elements called buckets; the various Application programming interfaces (APIs) allow to deal with the buckets. Inside one bucket, one can store:
\begin{itemize}
    \item a file system;
    \item a database;
    \item a disk image;
    \item in general, objects.
\end{itemize}
At the same time, every functionality that limits scalability is dropped. This includes:
\begin{itemize}
    \item hierarchical storage in folders/directories;
    \item permission inheritance;
    \item authorization based on groups/roles.
\end{itemize}
<checksums, ? slide 160>
\subsubsection{Block storage}
It is a level of abstraction for storage, where data is organized in blocks, where a block is a set of data with fixed size. When the size of our data exceeds one block, data is stored in multiple blocks. Whenever the data is not an exact multiple of block size, the last one is not filled, but it is left partly empty.
It is the important to choose the block size in an appropriate way, so that the empty space is kept at a minimum. Small block sizes reduce the amount of wasted storage space, but increase the number of blocks that must be read in order to get a constant amount of data. On the contrary, large blocks allow for less readings needed, but increase the wasted space. Whatever the chosen size is, block storage leads to space inefficiency, as data are rarely (if ever) exact multiples of block size. Then, why using block storage at all?
There are multiple reasons>
\begin{itemize}
    \item block storage allows variable size storage to become somewhat standardized and fixed size;
    \item it also allows a more effective load balancing of the storage (we just know the amount of blocks and not the sizes of each and every file);
    \item error correction algorithms are more easily implemented in a block storage, even across multiple servers. It also allows to have data correction with less wasted space compared to data replication;
    \item performance is proportional to the deployed resources, and less dependent on access patterns (since `hot blocks', i.e., the most frequently requested blocks, are scattered on multiple servers);
    \item software abstraction is identical to traditional hard disks, as a block is analogous to a hard disk sector. Block based storage can be actually mounted and seen as virtual hard disks; this is typical usage for VMs. Also, due to striping and replication across the various nodes of the system (basically what happens in RAID0 and RAID1 architectures for physical disks), performance and reliability increase. Block storage is better than physical hard disks since it provides more functionalities: for instance, virtual snapshots (allowing the creation of journals and rollbacks, that is, the ability of doing Ctrl+Z), but also resizing/renaming/copying etc. of disk images/partitions.
\end{itemize}
\subsection{Analytics}
It is an umbrella term, under which many different attempts at finding patterns in data are grouped. Analysis is typically done on large computer networks, which process large amounts of data.
Our main concern is actually on the data side of things rather than on pure computing. 
On this note, the data we face is usually unstructured, meaning it does not have a predefined data model nor is it organized. It is typically text based (documents, email, messages) and contains heterogeneous data (dates, numbers, assertions and so on). It usually contains irregularities, ambiguities and errors: every information is wrong until proven right. This makes unstructured data particularly hard to process using traditional programs.
A typical problem is that, in a distributed computing environment, data and CPUs are not necessarily in the same place. This separation can lead to severe inefficiencies, due to round trip times (i.e., the time between the message being sent and the acknowledgment of the receipt arriving) and latency in general.
In this framework, map-reduce schemes allow partial data processing directly on the site where data is stored (i.e., the data servers). This type of processing is easy enough to implement when the type of processing is uniform and well defined; however, difficulties arise when the processing interferes with the need of the server to provide data with high efficiency (the server is in the condition that it must provide two services at the same time, both computing and providing the data to other requests).
Map-reduce is often based on open source implementations of the Hadoop ecosystem. The analysis consists of:
\begin{itemize}
    \item a design splitting the processing between multiple parallel data processing and aggregation algorithms;
    \item a `map' program, distributed and running on every data node. The map operation performs filtering and data analysis;
    \item a `reduce' program, which aggregates the results coming from all the `map' jobs.
\end{itemize}
Of course, such a paradigm requires parallelization of the process among multiple servers, while also providing a way of managing all status, communications and data transfers between all components of the system (including redundancy and fault tolerance).
\subsection{Data replication}
Transferring information between pools, under many different scenarios (LAN and/or WAN, master/slave, multiple masters, and so on...).
\begin{itemize}
    \item One way (master/slave): the slave copy is read only, so that only the replication operation can alter its contents. Each operation involving the master can then be replicated on the slave. To ensure consistency between master and slave, at fixed times, the files are scanned and any difference is found and replicated (the scan can be at different levels, from simple metadata up to the content of each file).
    \item Two ways (multiple masters): all pools are read/write. Each write operation can be intercepted and replicated to other masters. This obviously gives rise to conflicts, which require handling: for example, possible policies can include defining a  `super-master', last mod wins, duplication on conflict (thus keeping everything). Once again, consistency is guaranteed by a scheduled scan at various levels.
    In multi master systems, something as simple as a file being present in one pool and not in the other, can be faced in two ways. The file could be copied in order to fill the void in one disk, or erased from where it is, as to bring everything to the same level. What should be preferred? One option may be remembering the last sync date and compare the file's modification/creation dates; another may be simply keeping a database of files of last sync and log all requests. In all of this, the possibility of pool errors must also be taken into account (maybe the file is absent in one of the pools just because that pool failed, and not because it was actually erased).
\end{itemize}
In general, replication errors are similar to those in the multi master case, and there are many possible policies re synchronize the pools. For instance, a reference pool may be defined, or a third copy may be employed.
When data is replicated, unique file identification is important, as it allows moving/renaming and so on. There are many options, like filename, filename+size+last write time, filename+size+last write time+create time, hash of file content, hash of file content+size+last write time, server-side file ID...
Aggregated transfer efficiency is bound to the size of the data we want to transfer (actually only true for storages that require sequential access). Datasets predefine the `access patterns' to data (different datasets may have different policies for accessing the data). In general, recalling multiple files from the same dataset guarantees maximum efficiency (no need of jumping from dataset to dataset, with the risk of hab=ving to change the access pattern halfway through). However, this is also bound to physical implementation: co-locating dataset files on the same media is better for tapes, while scattering datasets among various media is better in the case of disks, as it provides the maximum throughput. 
The scheduling of data replication can follow many approaches:
\begin{itemize}
    \item real-time, triggered by data changes: this way, data are always synchronized;
    \item at scheduled intervals: easier to keep track of the history and to make rollbacks, backups;
    \item manual replication;
    \item custom.
\end{itemize}
















.\chapter{Another Chapter}
\ShowChapterInfo{}
%% Text that introduces the chapter
\begin{chapterintro}  
  \lipsum[1]
\end{chapterintro}


\lipsum[2-12]

Here's citations to a paper\cite{bragason2015parachuterelease}

\printbibliography{}
\end{document}
%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% TeX-engine: xetex
%%% End:

